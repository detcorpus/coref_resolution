{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Udpiping and preprocessing\n",
    "\n",
    "What is here\n",
    "\n",
    "- we take a subset of texts from our data\n",
    "- use udpipe russian-gsd-ud-2.3-181115\n",
    "- convert conllu to pd.df\n",
    "- write each text in a separate .csv file\n",
    "- read from each file and preprocess them (add features, filter columns)\n",
    "- rewrite each df (text) in a separate .csv file that are in the ./data/texts_udpipe folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import ufal.udpipe\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the subset files\n",
    "# TODO: replace with loading data not from local files\n",
    "texts = {}\n",
    "\n",
    "for i in range(7, 13):\n",
    "    text_name = f\"detcorpus ({i}).txt\"\n",
    "    with open(f\"/Users/macbook/Downloads/detcorpus ({i}).txt\") as f:\n",
    "        texts[text_name] = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google implementation of udpipe\n",
    "# TODO: replace with custom udpiper\n",
    "\n",
    "udpipe_model = \"/Users/macbook/Downloads/russian-syntagrus-ud-2.3-181115.udpipe\"\n",
    "\n",
    "import ufal.udpipe\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, path):\n",
    "        \"\"\"Load given model.\"\"\"\n",
    "        self.model = ufal.udpipe.Model.load(path)\n",
    "        if not self.model:\n",
    "            raise Exception(\"Cannot load UDPipe model from file '%s'\" % path)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize the text and return list of ufal.udpipe.Sentence-s.\"\"\"\n",
    "        tokenizer = self.model.newTokenizer(self.model.DEFAULT)\n",
    "        if not tokenizer:\n",
    "            raise Exception(\"The model does not have a tokenizer\")\n",
    "        return self._read(text, tokenizer)\n",
    "\n",
    "    def read(self, text, in_format):\n",
    "        \"\"\"Load text in the given format (conllu|horizontal|vertical) and return list of ufal.udpipe.Sentence-s.\"\"\"\n",
    "        input_format = ufal.udpipe.InputFormat.newInputFormat(in_format)\n",
    "        if not input_format:\n",
    "            raise Exception(\"Cannot create input format '%s'\" % in_format)\n",
    "        return self._read(text, input_format)\n",
    "\n",
    "    def _read(self, text, input_format):\n",
    "        input_format.setText(text)\n",
    "        error = ufal.udpipe.ProcessingError()\n",
    "        sentences = []\n",
    "\n",
    "        sentence = ufal.udpipe.Sentence()\n",
    "        while input_format.nextSentence(sentence, error):\n",
    "            sentences.append(sentence)\n",
    "            sentence = ufal.udpipe.Sentence()\n",
    "        if error.occurred():\n",
    "            raise Exception(error.message)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        \"\"\"Tag the given ufal.udpipe.Sentence (inplace).\"\"\"\n",
    "        self.model.tag(sentence, self.model.DEFAULT)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        \"\"\"Parse the given ufal.udpipe.Sentence (inplace).\"\"\"\n",
    "        self.model.parse(sentence, self.model.DEFAULT)\n",
    "\n",
    "    def write(self, sentences, out_format):\n",
    "        \"\"\"Write given ufal.udpipe.Sentence-s in the required format (conllu|horizontal|vertical).\"\"\"\n",
    "\n",
    "        output_format = ufal.udpipe.OutputFormat.newOutputFormat(out_format)\n",
    "        output = ''\n",
    "        for sentence in sentences:\n",
    "            output += output_format.writeSentence(sentence)\n",
    "        output += output_format.finishDocument()\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 33s, sys: 883 ms, total: 1min 34s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from conllu import parse\n",
    "\n",
    "model = Model(udpipe_model)\n",
    "udpipe_sents = []\n",
    "\n",
    "for text in texts:    \n",
    "    sentences = model.tokenize(texts[text])\n",
    "    for s in sentences:\n",
    "        model.tag(s)\n",
    "        model.parse(s) \n",
    "    data = model.write(sentences, \"conllu\")\n",
    "    sents = parse(data)\n",
    "    udpipe_sents.append(sents)\n",
    "    \n",
    "# list of lists of lists: each element is udpipe output for a text \n",
    "# that is a list of words with their features\n",
    "udpipe_inds = [[(sent+1, udpipe_sents[text][sent][word]) \n",
    "               for sent in range(0, len(udpipe_sents[text])) \n",
    "               for word in range(0, len(udpipe_sents[text][sent]))]\n",
    "             for text in range(0, len(udpipe_sents))]\n",
    "\n",
    "# writing all the dfs of texts into separate .csvs\n",
    "for text in range(len(udpipe_inds)):\n",
    "    udpipe_df_inds = pd.DataFrame(udpipe_inds[text], columns = ['sent_id', 'word'])\n",
    "    udpipe_df = pd.DataFrame(pd.DataFrame(udpipe_df_inds)['word'].tolist())\n",
    "    udpipe_df = pd.merge(udpipe_df_inds['sent_id'], \n",
    "                     udpipe_df, left_index=True, right_index=True, how='right')\n",
    "    udpipe_df.to_csv(f\"./data/texts_udpipe/{text}_text.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: I would rather read all the features but it is computationally less efficient \n",
    "# than considering only those variables that are needed for rules\n",
    "# DON'T uncomment the following line\n",
    "# feature_sets = [{ind: list(vals['feats'].items())} for ind, vals in udpipe_df.iterrows() if vals['feats']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(folder_name):\n",
    "    files = [os.path.join(dirpath, filename)\n",
    "             for dirpath, dirnames, filenames in os.walk(f\"./{folder_name}\") \n",
    "             for filename in [f for f in filenames if f.endswith(\".csv\")]]\n",
    "    \n",
    "    for file in files:\n",
    "        \n",
    "        udpipe_df = pd.read_csv(file)\n",
    "        \n",
    "        # defining variables needed for rules\n",
    "        udpipe_df.feats = udpipe_df.feats.astype(str)\n",
    "        udpipe_df['anim'] = np.where(udpipe_df.feats.str.contains(\"'Animacy', 'Anim'\"), 1, 0)\n",
    "        udpipe_df['anim'] = np.where(udpipe_df.xpostag == \"PRP\", 1, udpipe_df['anim'])\n",
    "        udpipe_df['gender'] = np.where(udpipe_df.feats.str.contains(\"'Gender', 'Fem'\"), \"Fem\", np.nan)\n",
    "        udpipe_df['gender'] = np.where(udpipe_df.feats.str.contains(\"'Gender', 'Masc'\"), \"Masc\", udpipe_df['gender'])\n",
    "        udpipe_df['number'] = np.where(udpipe_df.feats.str.contains(\"'Number', 'Sing'\"), \"Sing\", np.nan)\n",
    "        udpipe_df['number'] = np.where(udpipe_df.feats.str.contains(\"'Number', 'Plur'\"), \"Plur\", udpipe_df['number'])\n",
    "\n",
    "        udpipe_df['pronoun_person'] = np.where(udpipe_df.upostag == \"PRON\", \"Non-deictic\", np.nan)\n",
    "        udpipe_df['pronoun_person'] = np.where((udpipe_df.upostag == \"PRON\") & \n",
    "                                        ((udpipe_df.feats.str.contains(\"'Person', '1'\") |\n",
    "                                          (udpipe_df.feats.str.contains(\"'Person', '2'\")))), \n",
    "                                         \"Deictic\", udpipe_df['pronoun_person'])\n",
    "\n",
    "        udpipe_df['hero'] = np.where((udpipe_df.anim == 1) & \n",
    "                                     (udpipe_df.deprel.str.contains(\"nsubj\") == True), \n",
    "                                     1, 0)\n",
    "\n",
    "        udpipe_df['uuid'] = [str(s) + str(h) \n",
    "                             for s,h in zip(\n",
    "                                 udpipe_df.sent_id.tolist(), \n",
    "                                 udpipe_df.id.tolist())]\n",
    "\n",
    "        udpipe_df['head_uuid'] = [str(s) + str(h) \n",
    "                                  for s,h in zip(\n",
    "                                      udpipe_df.sent_id.tolist(), \n",
    "                                      udpipe_df['head'].tolist())]\n",
    "        \n",
    "        udpipe_df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_data(\"/data/texts_udpipe\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
