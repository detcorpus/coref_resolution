{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HeadMatch and HeadMatchPro "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_data(folder_name):\n",
    "    files = [os.path.join(dirpath, filename)\n",
    "             for dirpath, dirnames, filenames in os.walk(f\"./{folder_name}\") \n",
    "             for filename in [f for f in filenames if f.endswith(\".csv\")]]\n",
    "    # returning list of dataframes and keeping only relevant features\n",
    "    return [pd.read_csv(file, dtype='str')[['sent_id', 'uuid', 'form', \n",
    "                               'lemma', 'upostag', 'gender', 'number', \n",
    "                               'hero', 'head_uuid', 'pronoun_person']] \n",
    "            for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "udpipe_dfs = reading_data('/data/texts_udpipe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about constructed features:\n",
    "\n",
    "**Hero** if animated and (pro)noun subject (deprel variable contains \"nsubj\");\n",
    "\n",
    "**Deictic** if pronoun and Person equals either 1 or 2, if not Deictic and pronoun then Other;\n",
    "\n",
    "**uuid** identification code of a token -- it is compose from text id, sentence id, and word id in the sentence\n",
    "\n",
    "**head_uuid** identification code of a head token. composed the same as *uuid* but for the head word of a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>uuid</th>\n",
       "      <th>form</th>\n",
       "      <th>lemma</th>\n",
       "      <th>upostag</th>\n",
       "      <th>gender</th>\n",
       "      <th>number</th>\n",
       "      <th>hero</th>\n",
       "      <th>head_uuid</th>\n",
       "      <th>pronoun_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>Спасибо</td>\n",
       "      <td>спасИБО</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>что</td>\n",
       "      <td>ЧТО</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sent_id uuid     form    lemma upostag gender number hero head_uuid  \\\n",
       "0       1   11  Спасибо  спасИБО   CCONJ    NaN    NaN    0        10   \n",
       "1       1   12        ,        ,   PUNCT    NaN    NaN    0        14   \n",
       "2       1   13      что      ЧТО   SCONJ    NaN    NaN    0        14   \n",
       "\n",
       "  pronoun_person  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            NaN  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udpipe_dfs[0].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all Noun Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_nps(udpipe_dfs):\n",
    "    \"\"\"\n",
    "    returns list of dfs of all NPs where head is hero: \n",
    "    each df stands for a separate text\n",
    "    each row is a NP where head token is a hero\n",
    "    all columns ending with _x and all the word characteristics are referring to the NP head \n",
    "    and those that end with _y are referring to the dependant word \n",
    "    \"\"\"\n",
    "    # returns list of dfs of all NPs where head is hero \n",
    "    # each df stands for a separate text\n",
    "    all_heros = [text[text.hero == \"1\"] for text in udpipe_dfs]\n",
    "    all_nps = [pd.merge(all_heros[text_ind], udpipe_dfs[text_ind][['uuid', 'head_uuid', 'form', 'lemma']], \n",
    "                        left_on='uuid', right_on='head_uuid', how='left')\n",
    "               for text_ind in range(len(udpipe_dfs))]\n",
    "    \n",
    "    for text_ind in range(len(udpipe_dfs)):\n",
    "        all_nps[text_ind].uuid_y = np.where(pd.isna(all_nps[text_ind].uuid_y) == True, \n",
    "                                     all_nps[text_ind].uuid_x, \n",
    "                                     all_nps[text_ind].uuid_y) \n",
    "        \n",
    "        all_nps[text_ind].head_uuid_y = np.where(pd.isna(all_nps[text_ind].head_uuid_y) == True, \n",
    "                                                 all_nps[text_ind].uuid_x, \n",
    "                                                 all_nps[text_ind].head_uuid_y)\n",
    "        all_nps[text_ind]['np_id'] = list(range(len(all_nps[text_ind])))\n",
    "    return all_nps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nps = get_all_nps(udpipe_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>uuid_x</th>\n",
       "      <th>form_x</th>\n",
       "      <th>lemma_x</th>\n",
       "      <th>upostag</th>\n",
       "      <th>gender</th>\n",
       "      <th>number</th>\n",
       "      <th>hero</th>\n",
       "      <th>head_uuid_x</th>\n",
       "      <th>pronoun_person</th>\n",
       "      <th>uuid_y</th>\n",
       "      <th>head_uuid_y</th>\n",
       "      <th>form_y</th>\n",
       "      <th>lemma_y</th>\n",
       "      <th>np_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>113</td>\n",
       "      <td>книги</td>\n",
       "      <td>КНИГА</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>Masc</td>\n",
       "      <td>Plur</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111</td>\n",
       "      <td>113</td>\n",
       "      <td>http://royallib.com</td>\n",
       "      <td>http://royallib.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>113</td>\n",
       "      <td>книги</td>\n",
       "      <td>КНИГА</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>Masc</td>\n",
       "      <td>Plur</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112</td>\n",
       "      <td>113</td>\n",
       "      <td>Все</td>\n",
       "      <td>ВЕСЬ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>113</td>\n",
       "      <td>книги</td>\n",
       "      <td>КНИГА</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>Masc</td>\n",
       "      <td>Plur</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114</td>\n",
       "      <td>113</td>\n",
       "      <td>автора</td>\n",
       "      <td>автора</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>113</td>\n",
       "      <td>книги</td>\n",
       "      <td>КНИГА</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>Masc</td>\n",
       "      <td>Plur</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112</td>\n",
       "      <td>113</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>81</td>\n",
       "      <td>Разбойники</td>\n",
       "      <td>Разбойники</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>Masc</td>\n",
       "      <td>Plur</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sent_id uuid_x      form_x     lemma_x upostag gender number hero  \\\n",
       "0       1    113       книги       КНИГА    NOUN   Masc   Plur    1   \n",
       "1       1    113       книги       КНИГА    NOUN   Masc   Plur    1   \n",
       "2       1    113       книги       КНИГА    NOUN   Masc   Plur    1   \n",
       "3       1    113       книги       КНИГА    NOUN   Masc   Plur    1   \n",
       "4       8     81  Разбойники  Разбойники    NOUN   Masc   Plur    1   \n",
       "\n",
       "  head_uuid_x pronoun_person uuid_y head_uuid_y               form_y  \\\n",
       "0          14            NaN    111         113  http://royallib.com   \n",
       "1          14            NaN    112         113                  Все   \n",
       "2          14            NaN    114         113               автора   \n",
       "3          14            NaN    112         113                    ,   \n",
       "4          82            NaN     81          81                  NaN   \n",
       "\n",
       "               lemma_y  np_id  \n",
       "0  http://royallib.com      0  \n",
       "1                 ВЕСЬ      1  \n",
       "2               автора      2  \n",
       "3                    ,      3  \n",
       "4                  NaN      4  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nps[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HeadMatch \n",
    "\n",
    "•\t HeadMatch: two NPs corefer if their heads are the same (only for nouns and\n",
    "deictic pronouns);\n",
    "\n",
    "•\t HeadMatchPro: like the previous one, only non-deictic pronouns are paired\n",
    "with the nearest NP that agrees in gender and number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headmatch_corefs(text_nps, window_size=3, pro=True):\n",
    "    \"\"\"\n",
    "    takes a dataframe of NPs with their udpipe features of one text and sets connection between two NPs if\n",
    "    - they have the same head for nouns and deictic pronouns\n",
    "    - if pro parameter is True: non-deictic pronouns are paired with the nearest NP with the same gender and number\n",
    "    \n",
    "    for pro=True: \n",
    "    window_size parameter is the number of sentences to look at while searching for coreferences\n",
    "    if window_size=3 and sentence id of NP is X then we will look for closest NP in [X-3:X+3] sentences\n",
    "    \n",
    "    returns list of sets of NP indexes in text_nps that are coreferenced for each text\n",
    "    \"\"\"\n",
    "    # get indexes of deictic NPs\n",
    "    deictic_nps = text_nps[['sent_id', 'np_id']][(text_nps.upostag == \"NOUN\") | \n",
    "                                                 (text_nps.pronoun_person == \"Deictic\")]\n",
    "    nps_by_sent = {}\n",
    "    for sent in list(deictic_nps.sent_id.unique()):\n",
    "        nps_by_sent[sent] = []\n",
    "        deictic_nps_sent = deictic_nps[deictic_nps.sent_id == sent]\n",
    "        nps_by_sent[sent].extend(deictic_nps_sent['np_id'])\n",
    "        \n",
    "    connections = [(np_x, np_y)\n",
    "                   for sent in list(nps_by_sent.keys()) \n",
    "                   for np_x in nps_by_sent[sent] \n",
    "                   for np_y in nps_by_sent[sent] \n",
    "                   if ((int(text_nps['head_uuid_y'][text_nps.np_id == np_x]) == \n",
    "                        int(text_nps['head_uuid_y'][text_nps.np_id == np_y])) &\n",
    "                       (np_x != np_y))]\n",
    "    \n",
    "    # TODO\n",
    "    #window_nps = [(vals['sent_id']-window_size, vals['sent_id']+window_size) for ind, vals in all_nps.iterrows()]\n",
    "\n",
    "    return connections\n",
    "\n",
    "def get_corefs_for_all_texts(all_nps, window_size=3, method=\"headmatch\", pro=True, folder_name=\"./data/texts_corefs\"):\n",
    "    \"\"\"\n",
    "    saves lists of sets (where each list of sets is list of NP coreferences for one text) to separate .pkl files \n",
    "    \"\"\"\n",
    "    iters = 0\n",
    "    for text_id in tqdm(range(len(all_nps))):\n",
    "        text_noun_phrases = all_nps[text_id]\n",
    "        connections = get_headmatch_corefs(text_nps=text_noun_phrases, window_size=3, pro=True)\n",
    "        with open(f'{folder_name}/{text_id}.pkl', 'wb') as f:\n",
    "            pickle.dump(connections, f)\n",
    "            \n",
    "    print(f\"Have saved all the {len(all_nps)} texts to the {folder_name} folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47879316336242119c33a64630ca3723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Have saved all the 6 texts to the ./data/texts_corefs folder\n"
     ]
    }
   ],
   "source": [
    "get_corefs_for_all_texts(all_nps=all_nps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/texts_corefs/0.pkl', 'rb') as f:\n",
    "#     test_corefs = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
