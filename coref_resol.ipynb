{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HeadMatch and HeadMatchPro "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_data(folder_name):\n",
    "    files = [os.path.join(dirpath, filename)\n",
    "             for dirpath, dirnames, filenames in os.walk(f\"./{folder_name}\") \n",
    "             for filename in [f for f in filenames if f.endswith(\".csv\")]]\n",
    "    # returning list of dataframes and keeping only relevant features\n",
    "    return [pd.read_csv(file, dtype='str')[['sent_id', 'uuid', 'form', \n",
    "                               'lemma', 'upostag', 'gender', 'number', \n",
    "                               'hero', 'head_uuid', 'pronoun_person']] \n",
    "            for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "udpipe_dfs = reading_data('/data/texts_udpipe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about constructed features:\n",
    "\n",
    "**Hero** if animated and (pro)noun subject (deprel variable contains \"nsubj\");\n",
    "\n",
    "**Deictic** if pronoun and Person equals either 1 or 2, if not Deictic and pronoun then Other;\n",
    "\n",
    "**uuid** identification code of a token -- it is compose from text id, sentence id, and word id in the sentence\n",
    "\n",
    "**head_uuid** identification code of a head token. composed the same as *uuid* but for the head word of a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>uuid</th>\n",
       "      <th>form</th>\n",
       "      <th>lemma</th>\n",
       "      <th>upostag</th>\n",
       "      <th>gender</th>\n",
       "      <th>number</th>\n",
       "      <th>hero</th>\n",
       "      <th>head_uuid</th>\n",
       "      <th>pronoun_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>Спасибо</td>\n",
       "      <td>спасибо</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sing</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>что</td>\n",
       "      <td>что</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sent_id uuid     form    lemma upostag gender number hero head_uuid  \\\n",
       "0       1   11  Спасибо  спасибо    NOUN    NaN   Sing    0        10   \n",
       "1       1   12        ,        ,   PUNCT    NaN    NaN    0        14   \n",
       "2       1   13      что      что   SCONJ    NaN    NaN    0        14   \n",
       "\n",
       "  pronoun_person  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udpipe_dfs[0].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all Noun Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_nps(udpipe_dfs):\n",
    "    \"\"\"\n",
    "    returns list of dfs of all NPs where head is hero: \n",
    "    each df stands for a separate text\n",
    "    each row is a NP where head token is a hero\n",
    "    all columns ending with _x and all the word characteristics are referring to the NP head \n",
    "    and those that end with _y are referring to the dependant word \n",
    "    \"\"\"\n",
    "    # returns list of dfs of all NPs where head is hero \n",
    "    # each df stands for a separate text\n",
    "    all_heros = [text[text.hero == \"1\"] for text in udpipe_dfs]\n",
    "    all_nps = [pd.merge(all_heros[text_ind], udpipe_dfs[text_ind][['uuid', 'head_uuid', 'form', 'lemma']], \n",
    "                        left_on='uuid', right_on='head_uuid', how='left')\n",
    "               for text_ind in range(len(udpipe_dfs))]\n",
    "    \n",
    "    for text_ind in range(len(udpipe_dfs)):\n",
    "        all_nps[text_ind].uuid_y = np.where(pd.isna(all_nps[text_ind].uuid_y) == True, \n",
    "                                     all_nps[text_ind].uuid_x, \n",
    "                                     all_nps[text_ind].uuid_y) \n",
    "        \n",
    "        all_nps[text_ind].head_uuid_y = np.where(pd.isna(all_nps[text_ind].head_uuid_y) == True, \n",
    "                                                 all_nps[text_ind].uuid_x, \n",
    "                                                 all_nps[text_ind].head_uuid_y)\n",
    "        all_nps[text_ind]['np_id'] = list(range(len(all_nps[text_ind])))\n",
    "    return all_nps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nps = get_all_nps(udpipe_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>uuid_x</th>\n",
       "      <th>form_x</th>\n",
       "      <th>lemma_x</th>\n",
       "      <th>upostag</th>\n",
       "      <th>gender</th>\n",
       "      <th>number</th>\n",
       "      <th>hero</th>\n",
       "      <th>head_uuid_x</th>\n",
       "      <th>pronoun_person</th>\n",
       "      <th>uuid_y</th>\n",
       "      <th>head_uuid_y</th>\n",
       "      <th>form_y</th>\n",
       "      <th>lemma_y</th>\n",
       "      <th>np_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>81</td>\n",
       "      <td>Разбойники</td>\n",
       "      <td>Разбойник</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>Masc</td>\n",
       "      <td>Plur</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>192</td>\n",
       "      <td>Мальчики</td>\n",
       "      <td>мальчик</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>Masc</td>\n",
       "      <td>Plur</td>\n",
       "      <td>1</td>\n",
       "      <td>191</td>\n",
       "      <td>NaN</td>\n",
       "      <td>194</td>\n",
       "      <td>192</td>\n",
       "      <td>новоткацкой</td>\n",
       "      <td>новоткацкая</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>206</td>\n",
       "      <td>мальчики</td>\n",
       "      <td>мальчик</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>Masc</td>\n",
       "      <td>Plur</td>\n",
       "      <td>1</td>\n",
       "      <td>207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>223</td>\n",
       "      <td>гости</td>\n",
       "      <td>гость</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>Masc</td>\n",
       "      <td>Plur</td>\n",
       "      <td>1</td>\n",
       "      <td>222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>223</td>\n",
       "      <td>223</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>231</td>\n",
       "      <td>Разбойники</td>\n",
       "      <td>Разбойник</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>Masc</td>\n",
       "      <td>Plur</td>\n",
       "      <td>1</td>\n",
       "      <td>232</td>\n",
       "      <td>NaN</td>\n",
       "      <td>231</td>\n",
       "      <td>231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sent_id uuid_x      form_x    lemma_x upostag gender number hero  \\\n",
       "0       8     81  Разбойники  Разбойник    NOUN   Masc   Plur    1   \n",
       "1      19    192    Мальчики    мальчик    NOUN   Masc   Plur    1   \n",
       "2      20    206    мальчики    мальчик    NOUN   Masc   Plur    1   \n",
       "3      22    223       гости      гость    NOUN   Masc   Plur    1   \n",
       "4      23    231  Разбойники  Разбойник    NOUN   Masc   Plur    1   \n",
       "\n",
       "  head_uuid_x pronoun_person uuid_y head_uuid_y       form_y      lemma_y  \\\n",
       "0          82            NaN     81          81          NaN          NaN   \n",
       "1         191            NaN    194         192  новоткацкой  новоткацкая   \n",
       "2         207            NaN    206         206          NaN          NaN   \n",
       "3         222            NaN    223         223          NaN          NaN   \n",
       "4         232            NaN    231         231          NaN          NaN   \n",
       "\n",
       "   np_id  \n",
       "0      0  \n",
       "1      1  \n",
       "2      2  \n",
       "3      3  \n",
       "4      4  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nps[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HeadMatch \n",
    "\n",
    "•\t HeadMatch: two NPs corefer if their heads are the same (only for nouns and\n",
    "deictic pronouns);\n",
    "\n",
    "•\t HeadMatchPro: like the previous one, only non-deictic pronouns are paired\n",
    "with the nearest NP that agrees in gender and number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headmatch_corefs(text_nps, window_size=3, pro=True):\n",
    "    \"\"\"\n",
    "    takes a dataframe of NPs with their udpipe features of one text and sets connection between two NPs if\n",
    "    - they have the same head for nouns and deictic pronouns\n",
    "    - if pro parameter is True: non-deictic pronouns are paired with the nearest NP with the same gender and number\n",
    "    \n",
    "    for pro=True: \n",
    "    window_size parameter is the number of sentences to look at while searching for coreferences\n",
    "    if window_size=3 and sentence id of NP is X then we will look for closest NP in [X-3:X+3] sentences\n",
    "    \n",
    "    returns list of sets of NP indexes in text_nps that are coreferenced for each text\n",
    "    \"\"\"\n",
    "    # get indexes of deictic NPs\n",
    "    deictic_nps = text_nps[['sent_id', 'np_id']][(text_nps.upostag == \"NOUN\") | \n",
    "                                                 (text_nps.pronoun_person == \"Deictic\")]\n",
    "    nps_by_sent = {}\n",
    "    for sent in list(deictic_nps.sent_id.unique()):\n",
    "        nps_by_sent[sent] = []\n",
    "        deictic_nps_sent = deictic_nps[deictic_nps.sent_id == sent]\n",
    "        nps_by_sent[sent].extend(deictic_nps_sent['np_id'])\n",
    "        \n",
    "    connections = [(np_x, np_y)\n",
    "                   for sent in list(nps_by_sent.keys()) \n",
    "                   for np_x in nps_by_sent[sent] \n",
    "                   for np_y in nps_by_sent[sent] \n",
    "                   if ((int(text_nps['head_uuid_y'][text_nps.np_id == np_x]) == \n",
    "                        int(text_nps['head_uuid_y'][text_nps.np_id == np_y])) &\n",
    "                       (np_x != np_y))]\n",
    "    \n",
    "    if pro:\n",
    "        non_deic = text_nps.np_id[text_nps.pronoun_person=='Non-deictic'].tolist()\n",
    "        windows_inds = [(int(vals['sent_id'])-window_size, int(vals['sent_id'])+window_size) \n",
    "                        for ind, vals in text_nps.iterrows() \n",
    "                        if vals.np_id in non_deic]\n",
    "        windows_words = [text_nps[text_nps.sent_id.astype(int).isin(range(window[0], window[1]+1))] \n",
    "                         for window in windows_inds]\n",
    "        nps_by_wind = [(vals_X['np_id'], (vals_X['np_id'], vals_Y['np_id']))\n",
    "                       for window in windows_words\n",
    "                       for ind_X, vals_X in window.iterrows()\n",
    "                       for ind_Y, vals_Y in window.iterrows() \n",
    "                       if ((vals_X.np_id in non_deic) or vals_X.upostag == \"NOUN\") \n",
    "                       and (vals_X.gender == vals_Y.gender) \n",
    "                       and (vals_X.number == vals_Y.number) \n",
    "                       and (vals_X.np_id != vals_Y.np_id)]\n",
    "\n",
    "        np_conns = {}\n",
    "        for np in nps_by_wind:\n",
    "            np_conns[np[0]] = []\n",
    "            for connection in nps_by_wind:\n",
    "                if np[0] == connection[0]:\n",
    "                    np_conns[np[0]].extend(connection[1])\n",
    "                    np_conns[np[0]].remove(np[0])\n",
    "        \n",
    "        # choosing the closests matching np\n",
    "        connections_pro = []\n",
    "        for i in list(np_conns.keys()):\n",
    "            np_x = i\n",
    "            np_y = min(np_conns[i], key=lambda x:abs(x-i))\n",
    "            connections_pro.append((np_x, np_y))\n",
    "\n",
    "        connections.extend(connections_pro)\n",
    "        connections = list(set(connections))\n",
    "\n",
    "    return connections\n",
    "\n",
    "def get_corefs_for_all_texts(all_nps, window_size=3, method=\"headmatch\", pro=True, folder_name=\"./data/texts_corefs\"):\n",
    "    \"\"\"\n",
    "    saves lists of sets (where each list of sets is list of NP coreferences for one text) to separate .pkl files \n",
    "    \"\"\"\n",
    "    iters = 0\n",
    "    for text_id in tqdm(range(len(all_nps))):\n",
    "        text_noun_phrases = all_nps[text_id]\n",
    "        connections = get_headmatch_corefs(text_nps=text_noun_phrases, window_size=3, pro=pro)\n",
    "        with open(f'{folder_name}/{text_id}_{method}_{pro}.pkl', 'wb') as f:\n",
    "            pickle.dump(connections, f)\n",
    "            \n",
    "    print(f\"Have saved all the {len(all_nps)} texts to the {folder_name} folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd29df72f5e341bd9603a9f560ae17af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Have saved all the 6 texts to the ./data/texts_corefs folder\n"
     ]
    }
   ],
   "source": [
    "get_corefs_for_all_texts(all_nps, window_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d53bbf065a94795b180ea9a9451cee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Have saved all the 6 texts to the ./data/texts_corefs folder\n"
     ]
    }
   ],
   "source": [
    "get_corefs_for_all_texts(all_nps, pro=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking how similar the headmatch and headmatchpro lists of coreferences are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013782542113323124\n",
      "0.05037783375314862\n",
      "0.007774538386783284\n",
      "0.007774538386783284\n",
      "0.021447721179624665\n",
      "0.06535947712418301\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "for text in range(6):\n",
    "    with open(f'./data/texts_corefs/{text}_headmatch_True.pkl', 'rb') as f:\n",
    "        head_true = pickle.load(f)\n",
    "\n",
    "    with open(f'./data/texts_corefs/{text}_headmatch_False.pkl', 'rb') as f:\n",
    "        head_false = pickle.load(f)\n",
    "\n",
    "    sm=difflib.SequenceMatcher(None,head_true,head_false)\n",
    "    print(sm.ratio())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
